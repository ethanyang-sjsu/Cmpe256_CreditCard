{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3aaa228-a6d9-4edc-bb34-3a76690fc177",
   "metadata": {},
   "source": [
    "Fourth sandbox. In sandbox 3, the plot of F1 against `train_size` seemed to have an elbow at around 0.6, which happened to be the `train_size` used for the original bootstrap training. This sandbox tests the following hypothesis:\n",
    "- if the bootstrap training was done with `train_size=0.1`, then the metrics generated across model predictions on a split generated from `train_test_split(train_size=?)` will have an elbow around `train_size=0.1`\n",
    "- if the bootstrap training was done with `train_size=0.2`, then the metrics generated across model predictions on a split generated from `train_test_split(train_size=?)` will have an elbow around `train_size=0.2`\n",
    "- and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c890c4a-0f5e-4ace-aba8-0d7891de3458",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37799f8e-18db-407b-9527-ec21213b34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f9af9d-2e08-406a-99af-8254e047ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "print(df.columns)\n",
    "print(df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b547ed25-82d7-4fa1-83f6-983ef14f76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df[df.columns[:-1]].values\n",
    "y_all = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e66615-d2ad-4dc9-abd5-a6ea4147d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_size(X_sample, y_sample, seed, train_size, verbose=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sample, y_sample,\n",
    "        train_size=train_size,\n",
    "        stratify=y_sample,\n",
    "        random_state=seed  # make less random\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Fitting model with\", train_size, \"of the data...\", end=' ')\n",
    "        start_time = time()\n",
    "    np.random.seed(seed)  # make less random\n",
    "    model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        end_time = time()\n",
    "        print(\"done in\", round(end_time-start_time), \"seconds.\")\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d328bc22-68d8-4ba5-96fe-7b015066a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc07e62c-1f94-48c8-8ab5-c0c43c5af244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df):\n",
    "    plt.subplot(1, 3, 1)\n",
    "    df[\"Precision\"].plot.barh()\n",
    "    plt.title(\"Precision\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    df[\"Recall\"].plot.barh()\n",
    "    plt.title(\"Recall\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    df[\"F1\"].plot.barh()\n",
    "    plt.title(\"F1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee238f38-3f16-483a-91ad-690b6808de6d",
   "metadata": {},
   "source": [
    "# Bootstrapping a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c3b0a4-bb80-403e-84a1-83c5796faa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kdnuggets.com/2023/03/bootstrapping.html\n",
    "def bootstrap(X, y, n_samples, train_size):\n",
    "    results = []\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        run = {}\n",
    "        np.random.seed(i)  # make less random\n",
    "        indices = np.random.choice(X.shape[0], size=X.shape[0], replace=True)\n",
    "        X_sample = X[indices, :]\n",
    "        y_sample = y[indices]\n",
    "\n",
    "        model, X_test, y_test = fit_with_size(X_sample, y_sample, i, train_size)\n",
    "        precision, recall, f1 = get_metrics(model, X_test, y_test)\n",
    "\n",
    "        run[\"Seed\"] = i\n",
    "        run[\"Precision\"] = precision\n",
    "        run[\"Recall\"] = recall\n",
    "        run[\"F1\"] = f1\n",
    "        run[\"Indices\"] = indices\n",
    "        run[\"Model\"] = model\n",
    "        results.append(run)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3734d-edd3-43e5-9944-e91849108a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping with size 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 0.1 of the data... done in 1 seconds.\n",
      "Fitting model with 0.2 of the data... done in 6 seconds.\n",
      "Fitting model with 0.3 of the data... done in 9 seconds.\n",
      "Fitting model with 0.4 of the data... done in 9 seconds.\n",
      "Fitting model with 0.5 of the data... done in 11 seconds.\n",
      "Fitting model with 0.6 of the data... done in 18 seconds.\n",
      "Fitting model with 0.7 of the data... done in 21 seconds.\n",
      "Fitting model with 0.8 of the data... done in 23 seconds.\n",
      "Done with size 0.1\n",
      "Bootstrapping with size 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:05<00:21,  5.36s/it]"
     ]
    }
   ],
   "source": [
    "sizes = (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)\n",
    "data = {}\n",
    "start_time = time()\n",
    "for s_boot in sizes:\n",
    "    print(\"Bootstrapping with size\", s_boot)\n",
    "    pred_df = bootstrap(X_all, y_all, 5, s_boot)\n",
    "    \n",
    "    best_idx = pred_df.sort_values(by=\"F1\", ascending=False).index[0]\n",
    "    bootstrapped_idx = pred_df.iloc[best_idx][\"Indices\"]\n",
    "    bootstrapped_seed = pred_df.iloc[best_idx][\"Seed\"]\n",
    "    \n",
    "    X_sample = X_all[bootstrapped_idx, :]\n",
    "    y_sample = y_all[bootstrapped_idx]\n",
    "\n",
    "    fit = []\n",
    "    for s_fit in sizes:\n",
    "        model, X_test, y_test = fit_with_size(X_sample, y_sample, bootstrapped_seed, s_fit, verbose=True)\n",
    "        precision, recall, f1 = get_metrics(model, X_test, y_test)\n",
    "        fit.append(f1)\n",
    "    data[s_boot] = fit\n",
    "    print(\"Done with size\", s_boot)\n",
    "end_time = time()\n",
    "print(\"That took\", round(end_time-start_time), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef41b0-dabf-42c7-8731-80b7780e8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(data)\n",
    "df_data[\"Size\"] = sizes\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e73783-f15d-4de7-990e-c736a3f5eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [plt.cm.viridis(s) for s in np.linspace(0, 1, num=len(sizes))]\n",
    "_ = df_data.plot(x=\"Size\", color=colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
